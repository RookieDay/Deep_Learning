{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fcfa823f940>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fcfa8235710>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fcfa81d2be0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 下面使用第一次开始使用的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算每一次层网络\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    wx_b = tf.matmul(inputs, weights) + biases\n",
    "    return wx_b if activation_function is None else activation_function(wx_b,)\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None, 28*28])\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return sess.run(accuracy, feed_dict={xs:v_xs, ys:v_ys})\n",
    "\n",
    "def model_mnist(learning_rate, avf=None):\n",
    "    layer1 = add_layer(xs, 784, 50, activation_function = avf)\n",
    "    layer2 = add_layer(layer1, 50, 50, activation_function = avf)\n",
    "    layer3 = add_layer(layer2, 50, 50, activation_function = avf)\n",
    "    layer4 = add_layer(layer3, 50, 50, activation_function = avf)\n",
    "    prediction = add_layer(layer4, 50, 10, activation_function = tf.nn.softmax)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), \n",
    "                                  reduction_indices=[1]))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    return (layer1, prediction, train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 ,  0.0941\n",
      "step: 1000 ,  0.5389\n",
      "step: 2000 ,  0.6202\n",
      "step: 3000 ,  0.6602\n",
      "step: 4000 ,  0.6872\n",
      "step: 5000 ,  0.7042\n",
      "step: 6000 ,  0.7199\n",
      "step: 7000 ,  0.7271\n",
      "step: 8000 ,  0.7371\n",
      "step: 9000 ,  0.7451\n",
      "step: 10000 ,  0.7504\n",
      "step: 11000 ,  0.7507\n",
      "step: 12000 ,  0.7578\n",
      "step: 13000 ,  0.7597\n",
      "step: 14000 ,  0.7633\n",
      "step: 15000 ,  0.7663\n",
      "step: 16000 ,  0.7679\n",
      "step: 17000 ,  0.7692\n",
      "step: 18000 ,  0.7711\n",
      "step: 19000 ,  0.7735\n",
      "step: 20000 ,  0.7761\n",
      "step: 21000 ,  0.7754\n",
      "step: 22000 ,  0.7771\n",
      "step: 23000 ,  0.7779\n",
      "step: 24000 ,  0.779\n",
      "step: 25000 ,  0.7806\n",
      "step: 26000 ,  0.7811\n",
      "step: 27000 ,  0.7842\n",
      "step: 28000 ,  0.7829\n",
      "step: 29000 ,  0.7832\n",
      "step: 30000 ,  0.785\n",
      "step: 31000 ,  0.7851\n",
      "step: 32000 ,  0.786\n",
      "step: 33000 ,  0.7851\n",
      "step: 34000 ,  0.787\n",
      "step: 35000 ,  0.7882\n",
      "step: 36000 ,  0.7863\n",
      "step: 37000 ,  0.7866\n",
      "step: 38000 ,  0.7884\n",
      "step: 39000 ,  0.7901\n",
      "step: 40000 ,  0.7878\n",
      "step: 41000 ,  0.7891\n",
      "step: 42000 ,  0.7881\n",
      "step: 43000 ,  0.7891\n",
      "step: 44000 ,  0.7893\n",
      "step: 45000 ,  0.7888\n",
      "step: 46000 ,  0.7902\n",
      "step: 47000 ,  0.7903\n",
      "step: 48000 ,  0.79\n",
      "step: 49000 ,  0.7905\n",
      "step: 0 ,  0.0986\n",
      "step: 1000 ,  0.5937\n",
      "step: 2000 ,  0.7106\n",
      "step: 3000 ,  0.7665\n",
      "step: 4000 ,  0.7972\n",
      "step: 5000 ,  0.8153\n",
      "step: 6000 ,  0.8338\n",
      "step: 7000 ,  0.8442\n",
      "step: 8000 ,  0.854\n",
      "step: 9000 ,  0.8633\n",
      "step: 10000 ,  0.8698\n",
      "step: 11000 ,  0.8753\n",
      "step: 12000 ,  0.8816\n",
      "step: 13000 ,  0.8837\n",
      "step: 14000 ,  0.8887\n",
      "step: 15000 ,  0.8901\n",
      "step: 16000 ,  0.8938\n",
      "step: 17000 ,  0.8965\n",
      "step: 18000 ,  0.8981\n",
      "step: 19000 ,  0.9003\n",
      "step: 20000 ,  0.9032\n",
      "step: 21000 ,  0.9047\n",
      "step: 22000 ,  0.9064\n",
      "step: 23000 ,  0.9083\n",
      "step: 24000 ,  0.9091\n",
      "step: 25000 ,  0.9114\n",
      "step: 26000 ,  0.9118\n",
      "step: 27000 ,  0.9126\n",
      "step: 28000 ,  0.9131\n",
      "step: 29000 ,  0.9157\n",
      "step: 30000 ,  0.9149\n",
      "step: 31000 ,  0.9162\n",
      "step: 32000 ,  0.9171\n",
      "step: 33000 ,  0.9191\n",
      "step: 34000 ,  0.9192\n",
      "step: 35000 ,  0.9195\n",
      "step: 36000 ,  0.9214\n",
      "step: 37000 ,  0.9216\n",
      "step: 38000 ,  0.9229\n",
      "step: 39000 ,  0.922\n",
      "step: 40000 ,  0.9221\n",
      "step: 41000 ,  0.923\n",
      "step: 42000 ,  0.925\n",
      "step: 43000 ,  0.9241\n",
      "step: 44000 ,  0.925\n",
      "step: 45000 ,  0.9245\n",
      "step: 46000 ,  0.9252\n",
      "step: 47000 ,  0.9257\n",
      "step: 48000 ,  0.9253\n",
      "step: 49000 ,  0.9272\n",
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n"
     ]
    }
   ],
   "source": [
    "# 解读1 不同激励函数模型\n",
    "activation_functions = [tf.nn.tanh, tf.nn.sigmoid, tf.nn.relu]\n",
    "models = {}\n",
    "for avf in activation_functions:\n",
    "    layer1, prediction, train_step = model_mnist(0.05,avf)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "    #     初始化我们创建的变量\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        steps = []\n",
    "        accuracy_s = []\n",
    "        for i in range(50000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(200)\n",
    "    #         训练模型 \n",
    "            sess.run(layer1,  feed_dict = {xs: batch_xs, ys: batch_ys})\n",
    "\n",
    "            sess.run(train_step, feed_dict = {xs: batch_xs, ys:batch_ys})\n",
    "            if i % 1000 ==0:\n",
    "                accuracy = compute_accuracy(mnist.test.images, mnist.test.labels)\n",
    "                accuracy_s.append(accuracy)\n",
    "                steps.append(i)\n",
    "                print (\"step:\",i,\", \",accuracy)\n",
    "    models[str(avf.__name__)] = {'steps':steps,'accuracy_s':accuracy_s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relu': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " 'sigmoid': {'accuracy_s': [0.0986,\n",
       "   0.59369999,\n",
       "   0.71060002,\n",
       "   0.7665,\n",
       "   0.79720002,\n",
       "   0.81529999,\n",
       "   0.83380002,\n",
       "   0.84420002,\n",
       "   0.85399997,\n",
       "   0.86330003,\n",
       "   0.86979997,\n",
       "   0.87529999,\n",
       "   0.88160002,\n",
       "   0.88370001,\n",
       "   0.88870001,\n",
       "   0.8901,\n",
       "   0.89380002,\n",
       "   0.89649999,\n",
       "   0.89810002,\n",
       "   0.90030003,\n",
       "   0.90319997,\n",
       "   0.90469998,\n",
       "   0.90640002,\n",
       "   0.90829998,\n",
       "   0.9091,\n",
       "   0.91140002,\n",
       "   0.91180003,\n",
       "   0.91259998,\n",
       "   0.9131,\n",
       "   0.91570002,\n",
       "   0.9149,\n",
       "   0.91619998,\n",
       "   0.91710001,\n",
       "   0.91909999,\n",
       "   0.9192,\n",
       "   0.91949999,\n",
       "   0.92140001,\n",
       "   0.92159998,\n",
       "   0.92290002,\n",
       "   0.92199999,\n",
       "   0.92210001,\n",
       "   0.92299998,\n",
       "   0.92500001,\n",
       "   0.92409998,\n",
       "   0.92500001,\n",
       "   0.92449999,\n",
       "   0.92519999,\n",
       "   0.92570001,\n",
       "   0.9253,\n",
       "   0.92720002],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " 'tanh': {'accuracy_s': [0.094099998,\n",
       "   0.53890002,\n",
       "   0.62019998,\n",
       "   0.6602,\n",
       "   0.68720001,\n",
       "   0.70420003,\n",
       "   0.71990001,\n",
       "   0.72710001,\n",
       "   0.73710001,\n",
       "   0.74510002,\n",
       "   0.75040001,\n",
       "   0.7507,\n",
       "   0.75779998,\n",
       "   0.7597,\n",
       "   0.7633,\n",
       "   0.76630002,\n",
       "   0.76789999,\n",
       "   0.76920003,\n",
       "   0.77109998,\n",
       "   0.77350003,\n",
       "   0.77609998,\n",
       "   0.77539998,\n",
       "   0.77710003,\n",
       "   0.77789998,\n",
       "   0.77899998,\n",
       "   0.78060001,\n",
       "   0.78109998,\n",
       "   0.78420001,\n",
       "   0.78289998,\n",
       "   0.78320003,\n",
       "   0.78500003,\n",
       "   0.78509998,\n",
       "   0.78600001,\n",
       "   0.78509998,\n",
       "   0.787,\n",
       "   0.78820002,\n",
       "   0.7863,\n",
       "   0.78659999,\n",
       "   0.78839999,\n",
       "   0.79009998,\n",
       "   0.78780001,\n",
       "   0.78909999,\n",
       "   0.7881,\n",
       "   0.78909999,\n",
       "   0.78930002,\n",
       "   0.7888,\n",
       "   0.7902,\n",
       "   0.79030001,\n",
       "   0.79000002,\n",
       "   0.79049999],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7a035bc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制不同学习函数 accuracy的曲线\n",
    "for avf in activation_functions:\n",
    "    plt.plot(models[str(avf.__name__)]['steps'],models[str(avf.__name__)]['accuracy_s'],label=str(avf.__name__))\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('per 1000 steps)')\n",
    "legend = plt.legend(loc='best',shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n",
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n",
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n",
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n"
     ]
    }
   ],
   "source": [
    "# 不同学习率 accuracy\n",
    "learning_rate = [0.2, 0.1, 0.05,0.01]\n",
    "models_lr = {}\n",
    "for lr in learning_rate:\n",
    "    layer1, prediction, train_step = model_mnist(lr,tf.nn.relu)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "    #     初始化我们创建的变量\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        steps = []\n",
    "        accuracy_s = []\n",
    "        for i in range(50000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(200)\n",
    "    #         训练模型 \n",
    "            sess.run(layer1,  feed_dict = {xs: batch_xs, ys: batch_ys})\n",
    "\n",
    "            sess.run(train_step, feed_dict = {xs: batch_xs, ys:batch_ys})\n",
    "            if i % 1000 ==0:\n",
    "                accuracy = compute_accuracy(mnist.test.images, mnist.test.labels)\n",
    "                accuracy_s.append(accuracy)\n",
    "                steps.append(i)\n",
    "                print (\"step:\",i,\", \",accuracy)\n",
    "    models_lr[str(lr)] = {'steps':steps,'accuracy_s':accuracy_s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.01': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " '0.05': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " '0.1': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " '0.2': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAHU1JREFUeJzt3XucV3W97/HXm5kAERRQLEQQDaSw\nxGTCUneSKOClyDYV2m6blbQtj9U+RzE7J/dWa9ttaxcfJictKxKvCbkfxiYVPWYqAyLeYcQLExYk\neMEoBD7nj/Ud/DHMwJphLX5zeT8fj99j1vqu71q/73eY4T1rfX/ruxQRmJmZFaFHtRtgZmZdh0PF\nzMwK41AxM7PCOFTMzKwwDhUzMyuMQ8XMzArjUDEzs8I4VMzMrDAOFTMzK0xttRuwO+y7774xfPjw\najfDzKxTWbRo0V8iYlBb9ukWoTJ8+HDq6+ur3Qwzs05F0vNt3ceXv8zMrDAOFTMzK4xDxczMCtMt\nxlTMzNpj48aNNDQ0sGHDhmo3pVR77LEHI0aMoGfPnrt8LIeKmVkrGhoaqK2tZfDgwUiqdnNKERGs\nX7+e5cuXc+ihh+7y8Xz5y8ysFRs2bKBv375dNlAAJNG3b182bNjAunXrdvl4DhUzsx3oyoHSRBKS\nmDt37i4fy6FiZmYArF69epeP4VAxM+vg7r77bo455hiOOuoofvjDH263/eqrr+bYY49lwoQJfPzj\nH6exsbEKrcw4VMzMOrDNmzdz4YUXMmvWLBYsWMCcOXNYtmzZNnXe9a53cccdd3DnnXdy8sknc8kl\nl1SptQ4VM7MO7eGHH2b48OEceOCB9OzZkylTpjBv3rxt6hx99NH06dMHgCOOOIIXX3yxGk0F/JFi\nM7NcLr9nJcvXFHu/yshBe/CVY4fusM6f/vQn9t9//63rgwcPZvHixa3Wv/766znuuOMKa2NbOVTM\nzDqwiNiurLVPpN1yyy0sXbqUW265pexmtcqhYmaWw87OKMoyePBgVq1atXX9xRdf5G1ve9t29e69\n916+//3vc+utt9KrV6/d2cRteEzFzKwDO/zww3n22Wd54YUX2LhxI3PmzGHixInb1Hn00UeZMWMG\nP/vZz9h3332r1NKMz1TMzDqw2tpavvGNb3D66aezefNmpk2bxqhRo/j2t7/NmDFjmDRpEpdccgmv\nv/4606dPB2DIkCFcd9111WlvVd7VzMxymzBhAhMmTNim7Pzzz9+6fOONN+7uJrXKl7/MzKwwDhUz\nMyuMQ8XMzArjUDEzs8I4VMzMrDAOFTMzK0ypoSJpsqSnJTVIuqCF7R+QtFjSJklTm207Q9Ly9Doj\nlfWR9F+SnpL0uKTLymy/mVlHsLOp7x944AEmTpzI0KFDuf3226vQwjeVFiqSaoArgROB0cBpkkY3\nq/YC8GngV832HQhcBBwJjAMukjQgbf5uRLwDeA9wtKQTy+qDmVm15Zn6fsiQIVxxxRWceuqpVWrl\nm8o8UxkHNETEiojYCMwGplRWiIjnImIpsKXZvpOA+RGxNiLWAfOByRHx14i4O+27EVgMHFBiH8zM\nqirP1PdDhw5l9OjR9OhR/RGNMu+oHwKsrFhvJDvzaO++QyorSOoPfAj4fksHkDQdmA4wbNiwnG9r\nZtayvX7/TWpferLQY27a5528evSFO6zT1qnvq63MWGtpbubt53Bux76SaoHrgR9ExIqWDhARMyOi\nLiLqBg0alPNtzcw6lrZMfd8RlHmm0ghUzhV9ALCqlbot7Tu+2b4LKtZnAssj4opdaJ+ZWW47O6Mo\nS96p7zuKMs9UFgIjJR0kqScwDZibc995wERJA9IA/cRUhqRLgb2BL5fQZjOzDiXP1PcdSWmhEhGb\ngHPIwuBJ4MaIeFzSxZI+DCDpvZIagY8BV0t6PO27FriELJgWAhdHxFpJBwBfI/s02WJJSyR9rqw+\nmJlVW+XU98ceeywf+tCHtk593zRgv2TJEsaOHctvfvMbZsyYwfjx46vWXrV0va6rqauri/r6+mo3\nw8w6mUWLFm0zSN6VrVq1irvuuovzzjtva5mkRRFR15bjVP/zZ2Zm1mU4VMzMrDAOFTMzK4xDxczM\nCuNQMTOzwjhUzMysMA4VM7MObmdT3//973/n85//PEcddRQnn3wyK1dmUyeuXLmSgw8+mOOPP57j\njz+eGTNmlN7WMqdpMTOzXdQ09f3s2bMZPHgwJ510EpMmTeKQQw7ZWuf666+nf//+3H///dx2221c\neumlXH311QAceOCB/O53v9tt7fWZiplZB5Zn6vt58+bxsY99DIBTTjmF++67r8WJKHcHn6mYmeVw\n1VNX8cxrzxR6zLf3eztnv+PsHdbJM/V9ZZ3a2lr22msv1q5dC8ALL7zACSecQL9+/ZgxYwZHHpn3\nCSTt41AxM+vA8kx931qd/fbbj4ULFzJw4ECWLl3KmWeeyYIFC+jXr19p7XWomJnlsLMzirLkmfq+\nqc7+++/Ppk2bePXVVxkwYACS6NWrFwCHHXYYw4cPZ8WKFYwZM6a09npMxcysA8sz9f3EiRO56aab\nALj99ts55phjkMRLL73E5s2bAXj++ed59tlnS38Srs9UzMw6sMqp7zdv3sy0adO2Tn0/ZswYJk2a\nxGmnnca5557LUUcdRf/+/bnqqqsAeOCBB/jOd75DbW0tPXr04LLLLmPAgAGlttdT35uZtcJT33vq\nezMzqyKHipmZFcahYmZmhXGomJlZYRwqZmZWGIeKmZkVxqFiZtbBtXfq+7Vr1zJ16lRGjBjBhRde\nuFva6lAxM+vAmqa+nzVrFgsWLGDOnDksW7ZsmzqVU9+fddZZXHrppQD07t2b8847j69//eu7rb0O\nFTOzDmxXpr7v06cPRx555Nb5v3YHT9NiZpbDX3/0IzY3FDv1fc2It9PnnHN2WGdXpr7fZ599Cm1v\nHj5TMTPrwHZl6vtq8JmKmVkOOzujKMuuTH1fDT5TMTPrwHZl6vtq8JmKmVkHtitT3wOMGzeO9evX\ns3HjRubNm8f111/PIYccUl57SzuymZkVYsKECUyYMGGbsvPPP3/rcu/evZk5c2aL+z700EOltq05\nX/4yM7PClBoqkiZLelpSg6QLWtj+AUmLJW2SNLXZtjMkLU+vMyrKx0p6NB3zB6rWhUMzM9tOaaEi\nqQa4EjgRGA2cJml0s2ovAJ8GftVs34HARcCRwDjgIklNH2W4CpgOjEyvySV1wcysxY/rdjURUVg/\nyzxTGQc0RMSKiNgIzAamVFaIiOciYimwpdm+k4D5EbE2ItYB84HJkgYDe0XEHyL7Dvwc+EiJfTCz\nbmyPPfZg/fr1XTpYIoLXXnuNN954o5DjlTlQPwRYWbHeSHbm0d59h6RXYwvlZmaFGzFiBMuWLWPV\nqlVV+4hu2SKCN954g2eeeYaamppdPl6ZodLSv0DeuG9t39zHlDSd7DIZw4YNy/m2ZmZv6tmzJ4ce\neiizZs3iz3/+M/369euS4RIRvPLKK4waNWqXj1Xm5a9GYGjF+gHAqlbq5t23MS3v9JgRMTMi6iKi\nbtCgQbkbbWZWSRIf/ehHGTVqFDU1NUjqcq+amhrGjBnD5Mm7PkRd5pnKQmCkpIOAPwLTgNNz7jsP\n+GbF4PxE4KsRsVbSa5LeBzwI/DOw/cMFzMwK1KdPH0455ZRqN6NTKO1MJSI2AeeQBcSTwI0R8bik\niyV9GEDSeyU1Ah8Drpb0eNp3LXAJWTAtBC5OZQBnAz8BGoBngDvK6oOZmbWNuvKnGprU1dVFfX19\ntZthZtapSFoUEXVt2cd31JuZWWEcKmZmVhiHipmZFcahYmZmhXGomJlZYRwqZmZWGIeKmZkVxqFi\nZmaFcaiYmVlhHCpmZlYYh4qZmRXGoWJmZoVxqJiZWWEcKmZmVhiHipmZFSZXqEi6RdLJkhxCZmbW\nqrwhcRXZo4CXS7pM0jtKbJOZmXVSuUIlIn4XEZ8EjgCeA+ZLul/SmZLeUmYDzcys88h9OUvSPsCn\ngc8BDwPfJwuZ+aW0zMzMOp3aPJUk3Qq8A/gF8KGIeDFtukGSH/5uZmZAzlABfhQRd7W0ISLqCmyP\nmZl1Ynkvf71TUv+mFUkDJH2hpDaZmVknlTdUzoqIl5tWImIdcFY5TTIzs84qb6j0kKSmFUk1QM9y\nmmRmZp1V3jGVecCNkn4MBPAvwG9La5WZmXVKeUNlBvB54GxAwH8DPymrUWZm1jnlCpWI2EJ2V/1V\n5TbHzMw6s7z3qYwE/gMYDfRuKo+Ig0tql5mZdUJ5B+p/SnaWsgn4IPBzshshzczMtsobKntExJ2A\nIuL5iPg34LjymmVmZp1R3oH6v6Vp75dLOgf4I7Bfec0yM7POKO+ZypeBPsC5wFjgn4AzdraTpMmS\nnpbUIOmCFrb3knRD2v6gpOGpvKekn0p6VNIjksZX7HNaKl8q6beS9s3ZBzMzK9lOQyXd6PjxiFgf\nEY0RcWZE/GNEPJBjvyuBE8kG+E+TNLpZtc8C6yJiBHA58K1UfhZARLwbOAH4nqQekmrJZkf+YEQc\nBiwFzsnbWTMzK9dOQyUiNgNjK++oz2kc0BARKyJiIzAbmNKszhTgurR8MzAhvc9o4M70/quBl4E6\nsntkBOyZ6u0FrGpju8zMrCR5x1QeBuZIugl4vakwIm7dwT5DgJUV643Aka3ViYhNkl4B9gEeAaZI\nmg0MJbvkNjQiHpJ0NvBoasdy4Is5+9BmN57+Xvqufn3nFc3MOqD1++3Jx3+1cLe+Z95QGQi8xLaf\n+ApgR6HS0plN5KxzLfBOoB54Hrgf2JSeMnk28B5gBfBD4KvApdu9uTQdmA4wbNiwHTTTzMyKkveO\n+jPbcexGsrOMJgew/aWqpjqNabxkb2BtRATwlaZKku4nOys5PLXnmVR+I7DdBwBSnZnATIC6urrm\nYZbL7k54M7POLu8d9T9l+7MMIuIzO9htITBS0kFkH0GeBpzerM5csk+R/QGYCtwVESGpD9k9Ma9L\nOgHYFBFPSNofGC1pUESsIRvEfzJPH8zMrHx5L3/dXrHcGziVnQyQpzGSc8hmOK4Bro2IxyVdDNRH\nxFzgGuAXkhqAtWTBA9k9MPMkbSELpE+lY66S9O/AvZLeILs09umcfTAzs5Ipu9LUxp2yGyF/FxGd\n4q76urq6qK+vr3YzzMw6FUmL2vrI+Lw3PzY3EvDot5mZbSPvmMprbDum8ieyZ6yYmZltlffTX/3K\nboiZmXV+uS5/STpV0t4V6/0lfaS8ZpmZWWeUd0zlooh4pWklIl4GLiqnSWZm1lnlDZWW6uX9OLKZ\nmXUTeUOlXtJ/Snq7pIMlXQ4sKrNhZmbW+eQNlf8BbARuAG4ENlDiRI5mZtY55f301+u0MseWmZlZ\nk7yf/povqX/F+gBJ88prlpmZdUZ5L3/tmz7xBUBErMPPqDczs2byhsoWSVunZUnPkm/XdPJmZtZ1\n5f1Y8NeA+yTdk9Y/QHoAlpmZWZO8A/W/lVRHFiRLgDlknwAzMzPbKu+Ekp8DvkT29MYlwPvIHqzV\nKaa+NzOz3SPvmMqXgPcCz0fEB8meEb+mtFaZmVmnlDdU/hYRfwOQ1CsingJGldcsMzPrjPIO1Dem\n+1RuA+ZLWsdOHidsZmbdT96B+lPT4r9JuhvYG/htaa0yM7NOqc0zDUfEPTuvZWZm3VF7n1FvZma2\nHYeKmZkVxqFiZmaFcaiYmVlhHCpmZlYYh4qZmRXGoWJmZoVxqJiZWWEcKmZmVhiHipmZFcahYmZm\nhXGomJlZYUoNFUmTJT0tqUHSBS1s7yXphrT9QUnDU3lPST+V9KikRySNr9inp6SZkpZJekrSP5bZ\nBzMzy6/NsxTnJakGuBI4AWgEFkqaGxFPVFT7LLAuIkZImgZ8C/gEcBZARLxb0n7AHZLeGxFbgK8B\nqyPiEEk9gIFl9cHMzNqmzDOVcUBDRKyIiI3AbGBKszpTgOvS8s3ABEkCRgN3AkTEauBloC7V+wzw\nH2nbloj4S4l9MDOzNigzVIYAKyvWG1NZi3UiYhPwCrAP8AgwRVKtpIOAscDQ9PRJgEskLZZ0k6S3\ntvTmkqZLqpdUv2bNmuJ6ZWZmrSozVNRCWeSscy1ZCNUDVwD3A5vILtcdAPw+Io4A/gB8t6U3j4iZ\nEVEXEXWDBg1qXw/MzKxNShtTIQuFoRXrB7D9c+2b6jRKqiV7TPHaiAjgK02VJN0PLAdeAv4K/Dpt\nuolsXMbMzDqAMs9UFgIjJR0kqScwDZjbrM5c4Iy0PBW4KyJCUh9JewJIOgHYFBFPpLD5DTA+7TMB\neAIzM+sQSjtTiYhNks4B5gE1wLUR8biki4H6iJgLXAP8QlIDsJYseAD2A+ZJ2gL8EfhUxaFnpH2u\nANYAZ5bVBzMzaxtlf/x3bXV1dVFfX1/tZpiZdSqSFkVE3c5rvsl31JuZWWEcKmZmVhiHipmZFcah\nYmZmhXGomJlZYRwqZmZWGIeKmZkVxqFiZmaFcaiYmVlhHCpmZlYYh4qZmRXGoWJmZoVxqJiZWWEc\nKmZmVhiHipmZFcahYmZmhXGomJlZYRwqZmZWGIeKmZkVxqFiZmaFcaiYmVlhHCpmZlYYh4qZmRXG\noWJmZoVxqJiZWWEcKmZmVhiHipmZFcahYmZmhXGomJlZYRwqZmZWGIeKmZkVptRQkTRZ0tOSGiRd\n0ML2XpJuSNsflDQ8lfeU9FNJj0p6RNL4FvadK+mxMttvZmZtU1qoSKoBrgROBEYDp0ka3azaZ4F1\nETECuBz4Vio/CyAi3g2cAHxP0ta2SvoosL6stpuZWfuUeaYyDmiIiBURsRGYDUxpVmcKcF1avhmY\nIElkIXQnQESsBl4G6gAk9QX+Fbi0xLabmVk7lBkqQ4CVFeuNqazFOhGxCXgF2Ad4BJgiqVbSQcBY\nYGja5xLge8Bfy2u6mZm1R5mhohbKImeda8lCqB64Argf2CTpcGBERPx6p28uTZdUL6l+zZo1bWu5\nmZm1S5mh0sibZxcABwCrWqsjqRbYG1gbEZsi4isRcXhETAH6A8uB9wNjJT0H3AccImlBS28eETMj\noi4i6gYNGlRgt8zMrDVlhspCYKSkgyT1BKYBc5vVmQuckZanAndFREjqI2lPAEknAJsi4omIuCoi\n9o+I4cAxwLKIGF9iH8zMrA1qyzpwRGySdA4wD6gBro2IxyVdDNRHxFzgGuAXkhqAtWTBA7AfME/S\nFuCPwKfKaqeZmRVHEc2HObqeurq6qK+vr3YzzMw6FUmLIqKuLfv4jnozMyuMQ8XMzArjUDEzs8I4\nVMzMrDAOFTMzK4xDxczMCuNQMTOzwjhUzMysMA4VMzMrjEPFzMwK41AxM7PCOFTMzKwwDhUzMyuM\nQ8XMzArjUDEzs8I4VMzMrDAOFTMzK4xDxczMCuNQMTOzwjhUzMysMA4VMzMrjEPFzMwK41AxM7PC\nKCKq3YbSSVoDPN/O3fcF/lJgczoL97t7cb+7l7z9PjAiBrXlwN0iVHaFpPqIqKt2O3Y397t7cb+7\nlzL77ctfZmZWGIeKmZkVxqGyczOr3YAqcb+7F/e7eymt3x5TMTOzwvhMxczMCuNQaYWkyZKeltQg\n6YJqt6c9JF0rabWkxyrKBkqaL2l5+joglUvSD1J/l0o6omKfM1L95ZLOqCgfK+nRtM8PJGn39rBl\nkoZKulvSk5Iel/SlVN6l+y6pt6SHJD2S+v3vqfwgSQ+mPtwgqWcq75XWG9L24RXH+moqf1rSpIry\nDvt7IalG0sOSbk/r3aXfz6WfxSWS6lNZ9X7WI8KvZi+gBngGOBjoCTwCjK52u9rRjw8ARwCPVZR9\nG7ggLV8AfCstnwTcAQh4H/BgKh8IrEhfB6TlAWnbQ8D70z53ACdWu8+pXYOBI9JyP2AZMLqr9z21\npW9afgvwYOrPjcC0VP5j4Oy0/AXgx2l5GnBDWh6dfuZ7AQel34Wajv57Afwr8Cvg9rTeXfr9HLBv\ns7Kq/az7TKVl44CGiFgRERuB2cCUKrepzSLiXmBts+IpwHVp+TrgIxXlP4/MA0B/SYOBScD8iFgb\nEeuA+cDktG2viPhDZD95P684VlVFxIsRsTgtvwY8CQyhi/c9tX99Wn1LegVwHHBzKm/e76bvx83A\nhPRX6BRgdkT8PSKeBRrIfic67O+FpAOAk4GfpHXRDfq9A1X7WXeotGwIsLJivTGVdQVvjYgXIfvP\nF9gvlbfW5x2VN7ZQ3qGkSxvvIfurvcv3PV0CWgKsJvuP4Rng5YjYlKpUtnVr/9L2V4B9aPv3oyO4\nAjgf2JLW96F79BuyPxz+W9IiSdNTWdV+1mvb2YmurqVrhl39Y3Kt9bmt5R2GpL7ALcCXI+LVHVwK\n7jJ9j4jNwOGS+gO/Bt7ZUrX0ta39a+mP0Kr3W9IpwOqIWCRpfFNxC1W7VL8rHB0RqyTtB8yX9NQO\n6pb+s+4zlZY1AkMr1g8AVlWpLUX7czqlJX1dncpb6/OOyg9oobxDkPQWskCZFRG3puJu0XeAiHgZ\nWEB23by/pKY/ICvburV/afveZJdL2/r9qLajgQ9Leo7s0tRxZGcuXb3fAETEqvR1NdkfEuOo5s96\ntQeZOuKL7AxuBdlgXdPA3KHVblc7+zKcbQfqv8O2A3jfTssns+0A3kOpfCDwLNng3YC0PDBtW5jq\nNg3gnVTt/qZ2ieza7xXNyrt034FBQP+0vAfw/4BTgJvYdsD6C2n5i2w7YH1jWj6UbQesV5ANVnf4\n3wtgPG8O1Hf5fgN7Av0qlu8HJlfzZ73q35SO+iL7lMQysmvSX6t2e9rZh+uBF4E3yP7i+CzZteM7\ngeXpa9MPjoArU38fBeoqjvMZskHLBuDMivI64LG0z49IN9NW+wUcQ3aKvhRYkl4ndfW+A4cBD6d+\nPwZ8PZUfTPYJnob0H22vVN47rTek7QdXHOtrqW9PU/Fpn47+e8G2odLl+536+Eh6Pd7Utmr+rPuO\nejMzK4zHVMzMrDAOFTMzK4xDxczMCuNQMTOzwjhUzMysMA4VszaS9A5Jf5D0d0n/q9m2Fmezbc+M\nuTnacWFRfao45ilNsxubtYdDxWwnKu7KbrIWOBf4brN6NWT3AJxINuPtaZJGp83fAi6PiJHAOrJ7\nhkhf10XECODyVC+vwkMF+C+yu9P7lHBs6wYcKtblSRou6SlJ16VnSNzc9J9melbEPWkyvnkVU1ss\nkPRNSfcAX6o8XkSsjoiFZDeVVmpxNtt2zphb2f7Bku5Nz8t4TNI/SLoM2COVzUr1/knZ81SWSLo6\nhRyS1kv6nqTFku6UNCiVnyvpifQ9mZ36FmTTu5zS3u+3dW8OFesuRgEzI+Iw4FXgC2l+sB8CUyNi\nLHAt8I2KffpHxLER8b2c79HaTK/tmTG30unAvIg4HBgDLImIC4ANEXF4RHxS0juBT5BNLng4sBn4\nZNp/T2BxRBwB3ANclMovAN6Tvif/UvF+9cA/5Oyz2TY8S7F1Fysj4vdp+Zdkl69+C7yLbGZXyOZ5\nerFinxva+B7tmek1zyywC4FrUwjeFhFLWthnAjAWWJj6sgdvTiK4hTf78kugaYLNpcAsSbcBt1Uc\nazWwfwvvYbZTDhXrLpr/R930n/3jEfH+VvZ5vY3v0dpMr38hzZibzkZamjG3sdmMuW82NOJeSR8g\nmwzwF5K+ExE/b/beAq6LiK/maGfT9+JksqeDfhj4P5IOTe3rDWzI1WOzZnz5y7qLYZKawuM04D6y\nSQMHNZVLeoukQ3fhPRYCI9MnvXqSzYA7N41T3A1MTfXOAOak5blpnbT9rmg2IZ+kA8meF/J/gWvI\nHhEN8EY6e4Fs0sCp6ZkaTc8oPzBt61Hx3qcD90nqAQyNiLvJHm7VH+ib6hxCNoGgWZv5TMW6iyeB\nMyRdTTZz61URsVHSVOAHkvYm+324gmy211ZJehvZuMNewBZJXyZ7Zvmrks4B5pFdSrs2IpqONQOY\nLelSspmEr0nl15CdfTSQnaFMa+EtxwPnSXoDWA/8cyqfCSyVtDiNq/xvsicA9iD7EMEXgefJzrgO\nlbSIbMzmE6l9v0z9Ftkn015Ox/0gkOeMx2w7nqXYurx078ftEfGuKjelKiStj4i+O68Jkt4K/Coi\nJpTcLOuifPnLzCoNA/5ntRthnZfPVMzMrDA+UzEzs8I4VMzMrDAOFTMzK4xDxczMCuNQMTOzwjhU\nzMysMP8fmenu3jpmFeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7a5cc1278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制不同学习率 accuracy的曲线\n",
    "for lr in learning_rate:\n",
    "    plt.plot(models_lr[str(lr)]['steps'],models_lr[str(lr)]['accuracy_s'],label=str(lr))\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('per 1000 steps)')\n",
    "legend = plt.legend(loc='best',shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 针对relu学习率 训练出的模型进行检查\n",
    "import math\n",
    "def add_layer_x(inputs, in_size, out_size, activation_function=None):\n",
    "    weights = tf.Variable(tf.truncated_normal([in_size, out_size],stddev = 0.1))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    wx_b = tf.matmul(inputs, weights) + biases\n",
    "    return wx_b if activation_function is None else activation_function(wx_b,)\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None, 28*28])\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "def compute_accuracy_x(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return sess.run(accuracy, feed_dict={xs:v_xs, ys:v_ys})\n",
    "\n",
    "def model_mnist_x(learning_rate, avf=None):\n",
    "    layer1 = add_layer_x(xs, 784, 50, activation_function = avf)\n",
    "    layer2 = add_layer_x(layer1, 50, 50, activation_function = avf)\n",
    "    layer3 = add_layer_x(layer2, 50, 50, activation_function = avf)\n",
    "    layer4 = add_layer_x(layer3, 50, 50, activation_function = avf)\n",
    "    prediction = add_layer_x(layer4, 50, 10, activation_function = tf.nn.softmax)\n",
    "    \n",
    "    loss_name = 'lr%s' % str(learning_rate) \n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), \n",
    "                                      reduction_indices=[1]))\n",
    "        tf.summary.scalar(loss_name, cross_entropy) \n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    return (layer1, prediction, train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f8f7de692b0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f8f7de69208>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f8f6e991be0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 ,  0.0981\n",
      "step: 1000 ,  0.9035\n",
      "step: 2000 ,  0.9412\n",
      "step: 3000 ,  0.9522\n",
      "step: 4000 ,  0.9537\n",
      "step: 5000 ,  0.9619\n",
      "step: 6000 ,  0.9584\n",
      "step: 7000 ,  0.9649\n",
      "step: 8000 ,  0.9659\n",
      "step: 9000 ,  0.9689\n",
      "step: 10000 ,  0.9676\n",
      "step: 11000 ,  0.9673\n",
      "step: 12000 ,  0.9692\n",
      "step: 13000 ,  0.9727\n",
      "step: 14000 ,  0.9705\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n"
     ]
    }
   ],
   "source": [
    "# 不同学习率 accuracy\n",
    "# learning_rate = [0.2, 0.1, 0.05,0.01]\n",
    "learning_rate = [0.05]\n",
    "models_lx = {}\n",
    "for lr in learning_rate:\n",
    "    layer1, prediction, train_step = model_mnist_x(lr,tf.nn.relu)\n",
    "    with tf.Session() as sess:\n",
    "    #     初始化我们创建的变量\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter('/tmp/mnist' + '/train',sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter('/tmp/mnist' + '/valid')\n",
    "        summaries = tf.summary.merge_all()  \n",
    "    \n",
    "        steps ,accuracy_s= [],[]\n",
    "        for i in range(50000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(200)\n",
    "\n",
    "            sess.run(layer1,  feed_dict = {xs: batch_xs, ys: batch_ys})\n",
    "            sess.run(train_step, feed_dict = {xs: batch_xs, ys:batch_ys})\n",
    "            if i % 1000 ==0:\n",
    "                accuracy = compute_accuracy_x(mnist.test.images, mnist.test.labels)\n",
    "                accuracy_s.append(accuracy)\n",
    "                steps.append(i)\n",
    "                \n",
    "                train_sum = sess.run(summaries,feed_dict = {xs: mnist.train.images[:i], ys:mnist.train.labels[:i]})  \n",
    "                train_writer.add_summary(train_sum, global_step=i)  \n",
    "                valid_sum = sess.run(summaries,feed_dict = {xs: mnist.validation.images, ys:mnist.validation.labels})  \n",
    "                valid_writer.add_summary(valid_sum, global_step=i)  \n",
    "                \n",
    "                print (\"step:\",i,\", \",accuracy)\n",
    "    models_lx[str(lr)] = {'steps':steps,'accuracy_s':accuracy_s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f6a2ea4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制不同学习率 accuracy的曲线\n",
    "learning_rate = [0.05]\n",
    "for lr in learning_rate:\n",
    "    plt.plot(models_lx[str(lr)]['steps'],models_lx[str(lr)]['accuracy_s'],label=str(lr))\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('per 1000 steps)')\n",
    "legend = plt.legend(loc='best',shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 命令行输入: tensorboard  --logdir=/tmp/mnist 查看结果\n",
    "# 浏览器输入:127.0.0.1:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 下面全部层应用了scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算每一次层网络\n",
    "# graph = tf.Graph()\n",
    "def add_layer_1(inputs, in_size, out_size, n_layer,activation_function=None):\n",
    "    layer_name = 'layer%s' % n_layer  \n",
    "    with tf.name_scope(layer_name):  \n",
    "        with tf.name_scope('weights'):  \n",
    "            weights = tf.Variable(tf.truncated_normal([in_size, out_size],stddev = 0.1))\n",
    "            tf.summary.histogram(layer_name + '/weights', weights)  \n",
    "        with tf.name_scope('biases'):  \n",
    "            biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "            tf.summary.histogram(layer_name + '/biases', biases)  \n",
    "        with tf.name_scope('wx_b'):  \n",
    "            wx_b = tf.matmul(inputs, weights) + biases\n",
    "            \n",
    "        outputs = wx_b if activation_function is None else activation_function(wx_b,)  \n",
    "        tf.summary.histogram(layer_name + '/outputs', outputs)  \n",
    "        return outputs\n",
    "    \n",
    "with tf.name_scope('inputs'):  \n",
    "    xs = tf.placeholder(tf.float32, [None, 28*28],name='xs')\n",
    "    ys = tf.placeholder(tf.float32, [None, 10],name='ys')\n",
    "\n",
    "\n",
    "def compute_accuracy_1(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return sess.run(accuracy, feed_dict={xs:v_xs, ys:v_ys})\n",
    "\n",
    "# def model_mnist_1(learning_rate, avf=None):\n",
    "#     layer1 = add_layer_1(xs, 784, 50, n_layer=1,activation_function = avf)\n",
    "#     layer2 = add_layer_1(layer1, 50, 50, n_layer=2, activation_function = avf)\n",
    "#     layer3 = add_layer_1(layer2, 50, 50, n_layer=3, activation_function = avf)\n",
    "#     layer4 = add_layer_1(layer3, 50, 50, n_layer=4, activation_function = avf)\n",
    "#     prediction = add_layer_1(layer4, 50, 10, n_layer=5, activation_function = tf.nn.softmax)\n",
    "\n",
    "#     with tf.name_scope('loss'):  \n",
    "\n",
    "#         cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = ys,logits = prediction))\n",
    "#         tf.summary.scalar('loss', cross_entropy)  \n",
    "\n",
    "#     with tf.name_scope('train'):       \n",
    "#         train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "#     return (layer1, prediction, train_step)\n",
    "\n",
    "\n",
    "layer1 = add_layer_1(xs, 784, 50, n_layer=1,activation_function = tf.nn.relu)\n",
    "layer2 = add_layer_1(layer1, 50, 50, n_layer=2, activation_function = tf.nn.relu)\n",
    "layer3 = add_layer_1(layer2, 50, 50, n_layer=3, activation_function = tf.nn.relu)\n",
    "layer4 = add_layer_1(layer3, 50, 50, n_layer=4, activation_function = tf.nn.relu)\n",
    "prediction = add_layer_1(layer4, 50, 10, n_layer=5, activation_function = tf.nn.softmax)\n",
    "\n",
    "with tf.name_scope('loss'):  \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = ys,logits = prediction))\n",
    "    tf.summary.scalar('loss', cross_entropy)  \n",
    "\n",
    "with tf.name_scope('train'):       \n",
    "    train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 ,  0.0884\n",
      "step: 1000 ,  0.1237\n",
      "step: 2000 ,  0.1135\n",
      "step: 3000 ,  0.1135\n",
      "step: 4000 ,  0.1135\n",
      "step: 5000 ,  0.1135\n",
      "step: 6000 ,  0.1135\n",
      "step: 7000 ,  0.1135\n",
      "step: 8000 ,  0.1135\n",
      "step: 9000 ,  0.1135\n",
      "step: 10000 ,  0.1135\n",
      "step: 11000 ,  0.1135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-6d1e4afa9ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                     \u001b[0maccuracy_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-6a13e9a7551d>\u001b[0m in \u001b[0;36mcompute_accuracy_1\u001b[0;34m(v_xs, v_ys)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcorrect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# def model_mnist_1(learning_rate, avf=None):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1295\u001b[0m                 run_metadata):\n\u001b[1;32m   1296\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1358\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 不同学习率 accuracy\n",
    "# learning_rate = [0.2, 0.1, 0.05,0.01]\n",
    "learning_rate = [0.05]\n",
    "models_1 = {}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        for lr in learning_rate:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "#             writer = tf.summary.FileWriter(\"/tmp/logs_min\", sess.graph) \n",
    "            train_writer = tf.summary.FileWriter('/tmp/mnist_x' + '/train',sess.graph)\n",
    "            valid_writer = tf.summary.FileWriter('/tmp/mnist_x' + '/valid')\n",
    "            summaries = tf.summary.merge_all()  \n",
    "\n",
    "            steps = []\n",
    "            accuracy_s = []\n",
    "\n",
    "            for i in range(50000):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(200)\n",
    "        #         训练模型 \n",
    "#                 sess.run(layer1,  feed_dict = {xs: batch_xs, ys: batch_ys})\n",
    "                sess.run([layer1,train_step], feed_dict = {xs: batch_xs, ys:batch_ys})\n",
    "                if i % 1000 ==0:\n",
    "                    accuracy = compute_accuracy_1(mnist.test.images, mnist.test.labels)\n",
    "                    accuracy_s.append(accuracy)\n",
    "                    steps.append(i)\n",
    "\n",
    "#                     result = sess.run(merged,feed_dict = {xs: batch_xs, ys:batch_ys}) \n",
    "#                     writer.add_summary(result, global_step=i)  \n",
    "                    train_sum = sess.run(summaries,feed_dict = {xs: mnist.train.images[:i], ys:mnist.train.labels[:i]})  \n",
    "                    train_writer.add_summary(train_sum, global_step=i)  \n",
    "                    valid_sum = sess.run(summaries,feed_dict = {xs: mnist.validation.images, ys:mnist.validation.labels})  \n",
    "                    valid_writer.add_summary(valid_sum, global_step=i)  \n",
    "\n",
    "                    print (\"step:\",i,\", \",accuracy)\n",
    "        models_1[str(lr)] = {'steps':steps,'accuracy_s':accuracy_s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
