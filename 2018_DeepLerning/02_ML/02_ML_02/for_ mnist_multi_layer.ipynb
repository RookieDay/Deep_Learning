{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算每一次层网络\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    wx_b = tf.matmul(inputs, weights) + biases\n",
    "    return wx_b if activation_function is None else activation_function(wx_b,)\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None, 28*28])\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return sess.run(accuracy, feed_dict={xs:v_xs, ys:v_ys})\n",
    "\n",
    "def model_mnist(learning_rate, avf=None):\n",
    "    layer1 = add_layer(xs, 784, 50, activation_function = avf)\n",
    "    layer2 = add_layer(layer1, 50, 50, activation_function = avf)\n",
    "    layer3 = add_layer(layer2, 50, 50, activation_function = avf)\n",
    "    layer4 = add_layer(layer3, 50, 50, activation_function = avf)\n",
    "    prediction = add_layer(layer4, 50, 10, activation_function = tf.nn.softmax)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), \n",
    "                                  reduction_indices=[1]))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    return (layer1, prediction, train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 ,  0.0941\n",
      "step: 1000 ,  0.5389\n",
      "step: 2000 ,  0.6202\n",
      "step: 3000 ,  0.6602\n",
      "step: 4000 ,  0.6872\n",
      "step: 5000 ,  0.7042\n",
      "step: 6000 ,  0.7199\n",
      "step: 7000 ,  0.7271\n",
      "step: 8000 ,  0.7371\n",
      "step: 9000 ,  0.7451\n",
      "step: 10000 ,  0.7504\n",
      "step: 11000 ,  0.7507\n",
      "step: 12000 ,  0.7578\n",
      "step: 13000 ,  0.7597\n",
      "step: 14000 ,  0.7633\n",
      "step: 15000 ,  0.7663\n",
      "step: 16000 ,  0.7679\n",
      "step: 17000 ,  0.7692\n",
      "step: 18000 ,  0.7711\n",
      "step: 19000 ,  0.7735\n",
      "step: 20000 ,  0.7761\n",
      "step: 21000 ,  0.7754\n",
      "step: 22000 ,  0.7771\n",
      "step: 23000 ,  0.7779\n",
      "step: 24000 ,  0.779\n",
      "step: 25000 ,  0.7806\n",
      "step: 26000 ,  0.7811\n",
      "step: 27000 ,  0.7842\n",
      "step: 28000 ,  0.7829\n",
      "step: 29000 ,  0.7832\n",
      "step: 30000 ,  0.785\n",
      "step: 31000 ,  0.7851\n",
      "step: 32000 ,  0.786\n",
      "step: 33000 ,  0.7851\n",
      "step: 34000 ,  0.787\n",
      "step: 35000 ,  0.7882\n",
      "step: 36000 ,  0.7863\n",
      "step: 37000 ,  0.7866\n",
      "step: 38000 ,  0.7884\n",
      "step: 39000 ,  0.7901\n",
      "step: 40000 ,  0.7878\n",
      "step: 41000 ,  0.7891\n",
      "step: 42000 ,  0.7881\n",
      "step: 43000 ,  0.7891\n",
      "step: 44000 ,  0.7893\n",
      "step: 45000 ,  0.7888\n",
      "step: 46000 ,  0.7902\n",
      "step: 47000 ,  0.7903\n",
      "step: 48000 ,  0.79\n",
      "step: 49000 ,  0.7905\n",
      "step: 0 ,  0.0986\n",
      "step: 1000 ,  0.5937\n",
      "step: 2000 ,  0.7106\n",
      "step: 3000 ,  0.7665\n",
      "step: 4000 ,  0.7972\n",
      "step: 5000 ,  0.8153\n",
      "step: 6000 ,  0.8338\n",
      "step: 7000 ,  0.8442\n",
      "step: 8000 ,  0.854\n",
      "step: 9000 ,  0.8633\n",
      "step: 10000 ,  0.8698\n",
      "step: 11000 ,  0.8753\n",
      "step: 12000 ,  0.8816\n",
      "step: 13000 ,  0.8837\n",
      "step: 14000 ,  0.8887\n",
      "step: 15000 ,  0.8901\n",
      "step: 16000 ,  0.8938\n",
      "step: 17000 ,  0.8965\n",
      "step: 18000 ,  0.8981\n",
      "step: 19000 ,  0.9003\n",
      "step: 20000 ,  0.9032\n",
      "step: 21000 ,  0.9047\n",
      "step: 22000 ,  0.9064\n",
      "step: 23000 ,  0.9083\n",
      "step: 24000 ,  0.9091\n",
      "step: 25000 ,  0.9114\n",
      "step: 26000 ,  0.9118\n",
      "step: 27000 ,  0.9126\n",
      "step: 28000 ,  0.9131\n",
      "step: 29000 ,  0.9157\n",
      "step: 30000 ,  0.9149\n",
      "step: 31000 ,  0.9162\n",
      "step: 32000 ,  0.9171\n",
      "step: 33000 ,  0.9191\n",
      "step: 34000 ,  0.9192\n",
      "step: 35000 ,  0.9195\n",
      "step: 36000 ,  0.9214\n",
      "step: 37000 ,  0.9216\n",
      "step: 38000 ,  0.9229\n",
      "step: 39000 ,  0.922\n",
      "step: 40000 ,  0.9221\n",
      "step: 41000 ,  0.923\n",
      "step: 42000 ,  0.925\n",
      "step: 43000 ,  0.9241\n",
      "step: 44000 ,  0.925\n",
      "step: 45000 ,  0.9245\n",
      "step: 46000 ,  0.9252\n",
      "step: 47000 ,  0.9257\n",
      "step: 48000 ,  0.9253\n",
      "step: 49000 ,  0.9272\n",
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n"
     ]
    }
   ],
   "source": [
    "activation_functions = [tf.nn.tanh, tf.nn.sigmoid, tf.nn.relu]\n",
    "models = {}\n",
    "for avf in activation_functions:\n",
    "    layer1, prediction, train_step = model_mnist(0.05,avf)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "    #     初始化我们创建的变量\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        steps = []\n",
    "        accuracy_s = []\n",
    "        for i in range(50000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(200)\n",
    "    #         训练模型 \n",
    "            sess.run(layer1,  feed_dict = {xs: batch_xs, ys: batch_ys})\n",
    "\n",
    "            sess.run(train_step, feed_dict = {xs: batch_xs, ys:batch_ys})\n",
    "            if i % 1000 ==0:\n",
    "                accuracy = compute_accuracy(mnist.test.images, mnist.test.labels)\n",
    "                accuracy_s.append(accuracy)\n",
    "                steps.append(i)\n",
    "                print (\"step:\",i,\", \",accuracy)\n",
    "    models[str(avf.__name__)] = {'steps':steps,'accuracy_s':accuracy_s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relu': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " 'sigmoid': {'accuracy_s': [0.0986,\n",
       "   0.59369999,\n",
       "   0.71060002,\n",
       "   0.7665,\n",
       "   0.79720002,\n",
       "   0.81529999,\n",
       "   0.83380002,\n",
       "   0.84420002,\n",
       "   0.85399997,\n",
       "   0.86330003,\n",
       "   0.86979997,\n",
       "   0.87529999,\n",
       "   0.88160002,\n",
       "   0.88370001,\n",
       "   0.88870001,\n",
       "   0.8901,\n",
       "   0.89380002,\n",
       "   0.89649999,\n",
       "   0.89810002,\n",
       "   0.90030003,\n",
       "   0.90319997,\n",
       "   0.90469998,\n",
       "   0.90640002,\n",
       "   0.90829998,\n",
       "   0.9091,\n",
       "   0.91140002,\n",
       "   0.91180003,\n",
       "   0.91259998,\n",
       "   0.9131,\n",
       "   0.91570002,\n",
       "   0.9149,\n",
       "   0.91619998,\n",
       "   0.91710001,\n",
       "   0.91909999,\n",
       "   0.9192,\n",
       "   0.91949999,\n",
       "   0.92140001,\n",
       "   0.92159998,\n",
       "   0.92290002,\n",
       "   0.92199999,\n",
       "   0.92210001,\n",
       "   0.92299998,\n",
       "   0.92500001,\n",
       "   0.92409998,\n",
       "   0.92500001,\n",
       "   0.92449999,\n",
       "   0.92519999,\n",
       "   0.92570001,\n",
       "   0.9253,\n",
       "   0.92720002],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " 'tanh': {'accuracy_s': [0.094099998,\n",
       "   0.53890002,\n",
       "   0.62019998,\n",
       "   0.6602,\n",
       "   0.68720001,\n",
       "   0.70420003,\n",
       "   0.71990001,\n",
       "   0.72710001,\n",
       "   0.73710001,\n",
       "   0.74510002,\n",
       "   0.75040001,\n",
       "   0.7507,\n",
       "   0.75779998,\n",
       "   0.7597,\n",
       "   0.7633,\n",
       "   0.76630002,\n",
       "   0.76789999,\n",
       "   0.76920003,\n",
       "   0.77109998,\n",
       "   0.77350003,\n",
       "   0.77609998,\n",
       "   0.77539998,\n",
       "   0.77710003,\n",
       "   0.77789998,\n",
       "   0.77899998,\n",
       "   0.78060001,\n",
       "   0.78109998,\n",
       "   0.78420001,\n",
       "   0.78289998,\n",
       "   0.78320003,\n",
       "   0.78500003,\n",
       "   0.78509998,\n",
       "   0.78600001,\n",
       "   0.78509998,\n",
       "   0.787,\n",
       "   0.78820002,\n",
       "   0.7863,\n",
       "   0.78659999,\n",
       "   0.78839999,\n",
       "   0.79009998,\n",
       "   0.78780001,\n",
       "   0.78909999,\n",
       "   0.7881,\n",
       "   0.78909999,\n",
       "   0.78930002,\n",
       "   0.7888,\n",
       "   0.7902,\n",
       "   0.79030001,\n",
       "   0.79000002,\n",
       "   0.79049999],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7a035bc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for avf in activation_functions:\n",
    "    plt.plot(models[str(avf.__name__)]['steps'],models[str(avf.__name__)]['accuracy_s'],label=str(avf.__name__))\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('per 1000 steps)')\n",
    "legend = plt.legend(loc='best',shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n",
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n",
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n",
      "step: 0 ,  0.098\n",
      "step: 1000 ,  0.098\n",
      "step: 2000 ,  0.098\n",
      "step: 3000 ,  0.098\n",
      "step: 4000 ,  0.098\n",
      "step: 5000 ,  0.098\n",
      "step: 6000 ,  0.098\n",
      "step: 7000 ,  0.098\n",
      "step: 8000 ,  0.098\n",
      "step: 9000 ,  0.098\n",
      "step: 10000 ,  0.098\n",
      "step: 11000 ,  0.098\n",
      "step: 12000 ,  0.098\n",
      "step: 13000 ,  0.098\n",
      "step: 14000 ,  0.098\n",
      "step: 15000 ,  0.098\n",
      "step: 16000 ,  0.098\n",
      "step: 17000 ,  0.098\n",
      "step: 18000 ,  0.098\n",
      "step: 19000 ,  0.098\n",
      "step: 20000 ,  0.098\n",
      "step: 21000 ,  0.098\n",
      "step: 22000 ,  0.098\n",
      "step: 23000 ,  0.098\n",
      "step: 24000 ,  0.098\n",
      "step: 25000 ,  0.098\n",
      "step: 26000 ,  0.098\n",
      "step: 27000 ,  0.098\n",
      "step: 28000 ,  0.098\n",
      "step: 29000 ,  0.098\n",
      "step: 30000 ,  0.098\n",
      "step: 31000 ,  0.098\n",
      "step: 32000 ,  0.098\n",
      "step: 33000 ,  0.098\n",
      "step: 34000 ,  0.098\n",
      "step: 35000 ,  0.098\n",
      "step: 36000 ,  0.098\n",
      "step: 37000 ,  0.098\n",
      "step: 38000 ,  0.098\n",
      "step: 39000 ,  0.098\n",
      "step: 40000 ,  0.098\n",
      "step: 41000 ,  0.098\n",
      "step: 42000 ,  0.098\n",
      "step: 43000 ,  0.098\n",
      "step: 44000 ,  0.098\n",
      "step: 45000 ,  0.098\n",
      "step: 46000 ,  0.098\n",
      "step: 47000 ,  0.098\n",
      "step: 48000 ,  0.098\n",
      "step: 49000 ,  0.098\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.2, 0.1, 0.05,0.01]\n",
    "models_lr = {}\n",
    "for lr in learning_rate:\n",
    "    layer1, prediction, train_step = model_mnist(lr,tf.nn.relu)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "    #     初始化我们创建的变量\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        steps = []\n",
    "        accuracy_s = []\n",
    "        for i in range(50000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(200)\n",
    "    #         训练模型 \n",
    "            sess.run(layer1,  feed_dict = {xs: batch_xs, ys: batch_ys})\n",
    "\n",
    "            sess.run(train_step, feed_dict = {xs: batch_xs, ys:batch_ys})\n",
    "            if i % 1000 ==0:\n",
    "                accuracy = compute_accuracy(mnist.test.images, mnist.test.labels)\n",
    "                accuracy_s.append(accuracy)\n",
    "                steps.append(i)\n",
    "                print (\"step:\",i,\", \",accuracy)\n",
    "    models_lr[str(lr)] = {'steps':steps,'accuracy_s':accuracy_s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.01': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " '0.05': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " '0.1': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]},\n",
       " '0.2': {'accuracy_s': [0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997,\n",
       "   0.097999997],\n",
       "  'steps': [0,\n",
       "   1000,\n",
       "   2000,\n",
       "   3000,\n",
       "   4000,\n",
       "   5000,\n",
       "   6000,\n",
       "   7000,\n",
       "   8000,\n",
       "   9000,\n",
       "   10000,\n",
       "   11000,\n",
       "   12000,\n",
       "   13000,\n",
       "   14000,\n",
       "   15000,\n",
       "   16000,\n",
       "   17000,\n",
       "   18000,\n",
       "   19000,\n",
       "   20000,\n",
       "   21000,\n",
       "   22000,\n",
       "   23000,\n",
       "   24000,\n",
       "   25000,\n",
       "   26000,\n",
       "   27000,\n",
       "   28000,\n",
       "   29000,\n",
       "   30000,\n",
       "   31000,\n",
       "   32000,\n",
       "   33000,\n",
       "   34000,\n",
       "   35000,\n",
       "   36000,\n",
       "   37000,\n",
       "   38000,\n",
       "   39000,\n",
       "   40000,\n",
       "   41000,\n",
       "   42000,\n",
       "   43000,\n",
       "   44000,\n",
       "   45000,\n",
       "   46000,\n",
       "   47000,\n",
       "   48000,\n",
       "   49000]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMysMP8fmenu3jpmFeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7a5cc1278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lr in learning_rate:\n",
    "    plt.plot(models_lr[str(lr)]['steps'],models_lr[str(lr)]['accuracy_s'],label=str(lr))\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('per 1000 steps)')\n",
    "legend = plt.legend(loc='best',shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算每一次层网络\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    wx_b = tf.matmul(inputs, weights) + biases\n",
    "    return wx_b if activation_function is None else activation_function(wx_b,)\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None, 28*28])\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return sess.run(accuracy, feed_dict={xs:v_xs, ys:v_ys})\n",
    "\n",
    "def model_mnist(learning_rate, avf=None):\n",
    "    layer1 = add_layer(xs, 784, 50, activation_function = avf)\n",
    "    layer2 = add_layer(layer1, 50, 50, activation_function = avf)\n",
    "    layer3 = add_layer(layer2, 50, 50, activation_function = avf)\n",
    "    layer4 = add_layer(layer3, 50, 50, activation_function = avf)\n",
    "    prediction = add_layer(layer4, 50, 10, activation_function = tf.nn.softmax)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), \n",
    "                                  reduction_indices=[1]))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    return (layer1, prediction, train_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
