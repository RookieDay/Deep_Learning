{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import input_data\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you need to change the directories to yours.\n",
    "train_dir = './PetImages/train/'\n",
    "test_dir = './PetImages/test/'\n",
    "train_logs_dir = './logs/train/'\n",
    "val_logs_dir = './logs/val'\n",
    "\n",
    "N_CLASSES = 2\n",
    "IMG_W = 208     # resize the image, if the input image is too large, training will be very slow.\n",
    "IMG_H = 208\n",
    "RATIO = 0.2     # take 20% of dataset as validation data\n",
    "BATCH_SIZE = 64\n",
    "CAPACITY = 2000\n",
    "MAX_STEP = 100 # with current parameters, it is suggested to use MAX_STEP>10k\n",
    "learning_rate = 0.0001 # with current parameters, it is suggested to use learning rate<0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "\n",
    "    train, train_label, val, val_label = input_data.get_files(train_dir, RATIO)\n",
    "    train_batch, train_label_batch = input_data.get_batch(train,\n",
    "                                                  train_label,\n",
    "                                                  IMG_W,\n",
    "                                                  IMG_H,\n",
    "                                                  BATCH_SIZE,\n",
    "                                                  CAPACITY)\n",
    "    val_batch, val_label_batch = input_data.get_batch(val,\n",
    "                                                  val_label,\n",
    "                                                  IMG_W,\n",
    "                                                  IMG_H,\n",
    "                                                  BATCH_SIZE,\n",
    "                                                  CAPACITY)\n",
    "\n",
    "    #scale the float to -2.5 to 2.5\n",
    "    train_batch = (1/(2*2.25)) * train_batch + 0.5\n",
    "\n",
    "    logits = model.inference(train_batch, BATCH_SIZE, N_CLASSES)\n",
    "    loss = model.losses(logits, train_label_batch)\n",
    "    train_op = model.trainning(loss, learning_rate)\n",
    "    acc = model.evaluation(logits, train_label_batch)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "    y_ = tf.placeholder(tf.int16, shape=[BATCH_SIZE])\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess= sess, coord=coord)\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(train_logs_dir, sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(val_logs_dir, sess.graph)\n",
    "\n",
    "        try:\n",
    "            for step in np.arange(MAX_STEP):\n",
    "                if coord.should_stop():\n",
    "                        break\n",
    "                tra_images,tra_labels = sess.run([train_batch, train_label_batch])\n",
    "                _, tra_loss, tra_acc = sess.run([train_op, loss, acc],\n",
    "                                                feed_dict={x:tra_images, y_:tra_labels})\n",
    "                if step % 50 == 0:\n",
    "                    print('Step %d, train loss = %.2f, train accuracy = %.2f%%' %(step, tra_loss, tra_acc*100.0))\n",
    "                    summary_str = sess.run(summary_op)\n",
    "                    train_writer.add_summary(summary_str, step)\n",
    "\n",
    "                if step % 200 == 0 or (step + 1) == MAX_STEP:\n",
    "                    val_images, val_labels = sess.run([val_batch, val_label_batch])\n",
    "                    val_loss, val_acc = sess.run([loss, acc],\n",
    "                                                 feed_dict={x:val_images, y_:val_labels})\n",
    "                    print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' %(step, val_loss, val_acc*100.0))\n",
    "                    summary_str = sess.run(summary_op)\n",
    "                    val_writer.add_summary(summary_str, step)\n",
    "\n",
    "                if step % 2000 == 0 or (step + 1) == MAX_STEP:\n",
    "                    checkpoint_path = os.path.join(train_logs_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "        try:\n",
    "            coord.join(threads)\n",
    "        except:\n",
    "            error_type, error_value, trace_back = sys.exc_info()\n",
    "            print(error_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test one image\n",
    "\n",
    "def get_one_image(file_dir):\n",
    "    \"\"\"\n",
    "    Randomly pick one image from test data\n",
    "    Return: ndarray\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    test =[]\n",
    "    for file in os.listdir(file_dir):\n",
    "        test.append(file_dir + file)\n",
    "    print('There are %d test pictures\\n' %(len(test)))\n",
    "\n",
    "    n = len(test)\n",
    "    ind = np.random.randint(0, n)\n",
    "    print(ind)\n",
    "    img_test = test[ind]\n",
    "\n",
    "    image = Image.open(img_test)\n",
    "    plt.imshow(image)\n",
    "    image = image.resize([208, 208])\n",
    "    image = np.array(image)\n",
    "    return image\n",
    "\n",
    "def test_one_image():\n",
    "    \"\"\"\n",
    "    Test one image with the saved models and parameters\n",
    "    \"\"\"\n",
    "    test_image = get_one_image(test_dir)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        BATCH_SIZE = 100\n",
    "        N_CLASSES = 2\n",
    "\n",
    "        image = tf.cast(test_image, tf.float32)\n",
    "        image = tf.image.per_image_standardization(image)\n",
    "        image = tf.reshape(image, [-1, 208, 208, 3])\n",
    "        logit = model.inference(image, BATCH_SIZE, N_CLASSES)\n",
    "\n",
    "        logit = tf.nn.softmax(logit)\n",
    "\n",
    "        x = tf.placeholder(tf.float32, shape=[208, 208, 3])\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            print(\"Reading checkpoints...\")\n",
    "            ckpt = tf.train.get_checkpoint_state(train_logs_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print('Loading success, global_step is %s' % global_step)\n",
    "            else:\n",
    "                print('No checkpoint file found')\n",
    "\n",
    "            prediction = sess.run(logit, feed_dict={x: test_image})\n",
    "            max_index = np.argmax(prediction)\n",
    "            if max_index==0:\n",
    "                print('This is a cat with possibility %.6f' %prediction[:, 0])\n",
    "            else:\n",
    "                print('This is a dog with possibility %.6f' %prediction[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 命令行输入: tensorboard  --logdir=/tmp/mnist 查看结果\n",
    "# 浏览器输入:127.0.0.1:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11309 cats\n",
      "There are 11238 dogs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable conv1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/home/gl/python_code/Deep_Learning/2018_DeepLerning/03_ML/cats-dogs/model.py\", line 19, in inference\n    initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n  File \"<ipython-input-3-e484448ef33e>\", line 20, in training\n    logits = model.inference(train_batch, BATCH_SIZE, N_CLASSES)\n  File \"<ipython-input-6-a380071f81d7>\", line 2, in <module>\n    training()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a380071f81d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# if __name__ == '__main__':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# test_one_image()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e484448ef33e>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_code/Deep_Learning/2018_DeepLerning/03_ML/cats-dogs/model.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(images, batch_size, n_classes)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                   initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n\u001b[0m\u001b[1;32m     20\u001b[0m         biases = tf.get_variable('biases',\n\u001b[1;32m     21\u001b[0m                                  \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    662\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 664\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable conv1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/home/gl/python_code/Deep_Learning/2018_DeepLerning/03_ML/cats-dogs/model.py\", line 19, in inference\n    initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n  File \"<ipython-input-3-e484448ef33e>\", line 20, in training\n    logits = model.inference(train_batch, BATCH_SIZE, N_CLASSES)\n  File \"<ipython-input-6-a380071f81d7>\", line 2, in <module>\n    training()\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "training()\n",
    "# test_one_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
