{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images, batch_size, n_classes):\n",
    "    \"\"\"\n",
    "    Build the model\n",
    "    Args:\n",
    "        images: image batch, 4D tensor, tf.float32, [batch_size, width, height, channels]\n",
    "    Returns:\n",
    "        output tensor with the computed logits, float, [batch_size, n_classes]\n",
    "    \"\"\"\n",
    "    #conv1, shape = [kernel size, kernel size, channels, kernel numbers]\n",
    "# 使用3*3的卷积核 3通道图像  第四个参数也代表出现使用多少个卷积特征图像 conv1 卷出16个特征值\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [3,3,3, 16],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[16],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')  #208*208*16\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "#         走一个relu函数 208*208*16\n",
    "        conv1 = tf.nn.relu(pre_activation, name= scope.name)\n",
    "        print('conv1: ',conv1.shape)\n",
    "\n",
    "    #pool1 and norm1\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "#         走一个max_pool  LRN(Local Response Normalization） 局部响应归一化\n",
    "\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1],strides=[1,2,2,1],\n",
    "                               padding='SAME', name='pooling1')\n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "                          beta=0.75,name='norm1')\n",
    "#         走一个max_pool 出来 108*108*16  strides为2\n",
    "        print('norm1: ',norm1.shape)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    #conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "#         继续使用16通道卷积 卷积出16个特征\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[3,3,16,16],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[16],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(norm1, weights, strides=[1,1,1,1],padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "#         再走一个relu\n",
    "        conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
    "#         出来104*104*16 strides为1\n",
    "        print('conv2: ',conv2.shape)\n",
    "        \n",
    "    #pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "                          beta=0.75,name='norm2')\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1],\n",
    "                               padding='SAME',name='pooling2')\n",
    "#         走一个max_pool 出来 104*104*16 步长为1\n",
    "        print('pool2: ',pool2.shape)\n",
    "        \n",
    "# There are 11309 cats\n",
    "# There are 11238 dogs\n",
    "# conv1:  (64, 208, 208, 16)\n",
    "# norm1:  (64, 104, 104, 16)\n",
    "# conv2:  (64, 104, 104, 16)\n",
    "# pool2:  (64, 104, 104, 16)\n",
    "# local3:  (64, 128)\n",
    "# local4:  (64, 128)\n",
    "        \n",
    "    #local3 全链接层 128个神经元\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value  #dim 64\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[dim,128],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[128],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "        print('local3: ',local3.shape)\n",
    "\n",
    "    #local4 全链接层\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[128,128],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[128],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')\n",
    "        dropout3 = tf.nn.dropout(local4, 0.6)\n",
    "        print('local4: ',local4.shape)\n",
    "\n",
    "\n",
    "    # softmax\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = tf.get_variable('softmax_linear',\n",
    "                                  shape=[128, n_classes],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[n_classes],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        softmax_linear = tf.add(tf.matmul(dropout3, weights), biases, name='softmax_linear')\n",
    "\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "def losses(logits, labels):\n",
    "    \"\"\"\n",
    "    Compute loss from logits and labels\n",
    "    Args:\n",
    "        logits: logits tensor, float, [batch_size, n_classes]\n",
    "        labels: label tensor, tf.int32, [batch_size]\n",
    "    Returns:\n",
    "        loss tensor of float type\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits\\\n",
    "                        (logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name+'/loss', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降 使得loss最小化\n",
    "def trainning(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Training ops, the Op returned by this function is what must be passed to\n",
    "        'sess.run()' call to cause the model to train.\n",
    "    Args:\n",
    "        loss: loss tensor, from losses()\n",
    "    Returns:\n",
    "        train_op: The op for trainning\n",
    "    \"\"\"\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step= global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算accuracy\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of the logits at predicting the label.\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "        range [0, NUM_CLASSES).\n",
    "    Returns:\n",
    "        A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "        that were predicted correctly.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        tf.summary.scalar(scope.name+'/accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
